{"id":"doc_001","title":"Machine Learning Basics","source":"Custom Notes","text":"Machine learning is a field of artificial intelligence that enables systems to learn patterns from data and make decisions with minimal human intervention. The main categories are supervised learning, unsupervised learning, and reinforcement learning. In supervised learning, models learn from labeled examples to predict outcomes. Common algorithms include linear regression, logistic regression, decision trees, random forests, and support vector machines. In unsupervised learning, the model discovers structure from unlabeled data, such as clustering with K-means or dimensionality reduction with PCA. Reinforcement learning focuses on agents that learn by interacting with an environment and receiving rewards."}
{"id":"doc_002","title":"Neural Networks Overview","source":"Custom Notes","text":"Neural networks are computational models inspired by the human brain. They consist of layers of neurons that transform input data into output predictions. A basic network includes an input layer, one or more hidden layers, and an output layer. Training is typically performed using backpropagation, where the model computes a loss function and updates weights using gradient descent. Deep learning refers to neural networks with many layers, enabling them to learn complex features from data such as images, audio, and text."}
{"id":"doc_003","title":"Natural Language Processing (NLP)","source":"Custom Notes","text":"Natural Language Processing is the branch of AI that enables computers to understand, interpret, and generate human language. Traditional NLP approaches relied on rules and statistical methods such as n-grams and TF-IDF. Modern NLP uses neural networks and transformer architectures. Typical tasks include text classification, sentiment analysis, named entity recognition, machine translation, summarization, and question answering."}
{"id":"doc_004","title":"Transformers and Attention","source":"Custom Notes","text":"Transformers are deep learning models designed for sequence data. They are built primarily on the attention mechanism, which allows the model to focus on relevant parts of the input when producing output. Self-attention computes relationships between all tokens in the sequence, capturing contextual meaning. Transformers improve parallelization and outperform recurrent networks on many language tasks. Famous transformer-based models include BERT, GPT, T5, and BART."}
{"id":"doc_005","title":"Text Summarization","source":"Custom Notes","text":"Text summarization is the task of producing a shorter version of a document while preserving key meaning. Extractive summarization selects important sentences from the original text, while abstractive summarization generates new sentences that paraphrase the content. Transformer models such as BART and T5 perform strongly on abstractive summarization. Evaluation metrics include ROUGE, which measures overlap between generated and reference summaries."}
{"id":"doc_006","title":"Question Answering (QA)","source":"Custom Notes","text":"Question answering systems return answers to user questions based on a given context or a large knowledge source. Extractive QA finds a span of text in the context that answers the question. Common datasets include SQuAD. Modern extractive QA models are often based on BERT-like encoders such as RoBERTa. The output usually includes the answer text and a confidence score."}
{"id":"doc_007","title":"Generative AI","source":"Custom Notes","text":"Generative AI refers to models that can generate text, images, or other media. Large Language Models (LLMs) are generative models trained on massive datasets. They can perform tasks like summarization, translation, writing, and reasoning. Generative AI raises concerns related to hallucination, bias, privacy, and responsible AI usage. Common techniques to improve reliability include retrieval-augmented generation and human feedback alignment."}
{"id":"doc_008","title":"Model Evaluation and Metrics","source":"Custom Notes","text":"Evaluation in machine learning depends on the task. For classification, common metrics include accuracy, precision, recall, and F1-score. For regression, metrics include mean squared error and mean absolute error. For NLP tasks, metrics include BLEU for translation and ROUGE for summarization. Beyond metrics, practical evaluation includes error analysis, robustness testing, and fairness assessment."}
{"id":"doc_009","title":"Ethics in AI","source":"Custom Notes","text":"Ethics in AI focuses on fairness, accountability, transparency, and privacy. Bias can appear in datasets and models, leading to unfair outcomes for certain groups. Transparency refers to explaining how decisions are made, while accountability ensures responsibility for AI outcomes. Privacy concerns arise when training data includes personal information. Ethical AI encourages responsible deployment and monitoring to reduce harm."}
{"id":"doc_010","title":"AI Project Workflow","source":"Custom Notes","text":"A typical AI project workflow includes problem definition, data collection, preprocessing, exploratory data analysis, model selection, training, evaluation, and deployment. In NLP applications, it also includes text cleaning, tokenization, and model inference. For deployment, lightweight web apps like Streamlit or Gradio can provide user interfaces, while FastAPI can expose endpoints for integration. Good documentation and version control improve maintainability and reproducibility."}
